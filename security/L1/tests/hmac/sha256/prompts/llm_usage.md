# 大模型辅助使用记录

## 基本信息

- **模型名称**：OpenAI codex，juice 200
- **提供方 / 访问方式**：OpenAI API ([api.openai.com](http://api.openai.com/))
- **使用日期**：2025-10-26
- **项目名称**：HMAC-SHA256 L1 算子优化

---

## 使用场景 1**：性能瓶颈分析**

在开始优化前，我们首先在AI指导下完成了性能瓶颈的分析。这一阶段的主要目标是确保优化方向正确，并为后续优化提供参考基准和对比材料。

### **Prompt设计：**

我正在优化 Vitis 库中的 security::hmac::sha256 算子。

这是官方 SHA256算子的核心代码：

- `// --- 贴上官方原始核心代码 ---*`

这是它未经优化的 HLS 综合报告摘要：

`// --- 基准 HLS 报告中的 P&R---`

**【目标】**

我的优化目标是实现理论上的最高吞吐率，即降低端到端延迟，保持主循环流水线启动间隔（II）为 1。资源消耗是次要考虑因素。

**【诊断问题】**

1. 请解读这份报告。基于代码和报告，**根本原因**是什么导致了性能不佳（ Latency 过高）？
2. 代码中是否存在特定的**数据依赖**（如循环携带依赖）或**存储瓶颈**（如单端口 BRAM 访问冲突）？请指出具体是哪几行代码导致了这些问题。

### 模型输出摘要

llm准确地分析了P&R表格，指出当前 64 轮内环已做到 II=1，性能瓶颈主要是每拍组合路径过长（T1/T2 链路：旋转/异或 + 多操作数加法），其次是消息调度 W[t] 计算的四操作数长链。

### 人工审核与采纳情况

模型提供的瓶颈分析系统且专业，帮助我构建了初步的优化思路。

---

## 使用场景 2**：**优化策略制定

在了解算子优化瓶颈后，我在llm指导下完成了HLS 性能优化策略制定和代码重构方案探索。

### **Prompt设计：**

你好，我们将继续进行一个高层次综合（HLS）优化任务。请你继续扮演一名资深的 FPGA HLS 专家。

核心优化目标与评分 (不变):

- **最终评分** = **Execution Time = Estimated Clock Period(ns) × Cosim Latency(cycles)**
- **时序约束**: 若 **Slack ≤ 0 扣 10 分**。其中 Slack = Target × 0.9 − Estimated。
- **资源约束**: 资源使用率（LUT, FF, BRAM, DSP）超出 100% = 该题 0 分。

新的核心战略与指导思想:

1. **战略重心转移**: 我们之前的策略（在固定 Latency 下压榨 Estimated）已遇到瓶颈。现在的**首要任务**是：**在 `Estimated` 时钟周期不显著恶化的前提下，以最低的 `Latency` (执行周期数) 为核心优化目标。**
2. **激进的“资源换性能”**: 为了达成最低 `Latency`，我们采取**不计成本的资源换性能策略**。你可以大胆提出使用更多并行计算、深度流水线、数据复制、乒乓操作等任何消耗资源但能降低周期的方案。
3. **鼓励算法级重构**: 我们鼓励对 **`top` 函数内部调用的子函数进行大规模的算法级重构**。这意味着可以彻底改变原有函数的实现逻辑、数据流路径和计算方式，只要最终的功能等价即可。
4. **遵循官方最佳实践**: 所有的优化建议和代码实现，都应优先参考 **Xilinx/AMD 官方文档，特别是 UG1399 (Vitis HLS 用户指南)** 中关于延迟优化 (Latency Optimization)、数据流 (Dataflow)、循环依赖性 (Loop Dependencies) 等章节的官方推荐做法。

严格的开发限制 (最重要，保持不变):

- **工具与环境**: Xilinx Vitis HLS 2024.2, WSL 环境。
- **代码修改范围**: **只能**修改 Vitis Libraries 中 `L1/include/` 目录下的核心算法头文件（`.hpp`）。
- **测试平台不变**: **绝对不能**修改 `L1/tests/` 或 `L2/tests/` 目录下的任何测试文件。
- ✅ **允许**: 修改 `.hpp` 里的任何内部结构：重构子函数、使用 `#pragma HLS DATAFLOW`、`PIPELINE`、`UNROLL`、`ARRAY_PARTITION`、`BIND_STORAGE` 等。
- ✅ **允许**: 为了数据流或并行化，在函数内部复制算子/寄存器、增加缓存/FIFO。
- ✅ **允许**: 修改 Tcl/Makefile 中的 `create_clock -period` 以探索时序边界，但不允许tcl的其他内容
- ❌ **禁止**: 修改 `top` 函数的**端口**（名称、数量、类型、位宽、顺序）。
- ❌ **禁止**: 修改最终的输出结果，必须与原始算法在功能上完全等价。

### 模型输出摘要

模型接收了新的优化战略，并提出了一系列以降低执行周期（Latency）为核心的微架构优化方案。主要建议包括：

- **方案A**：将 `generateMsgSchedule` 函数中分离的两个循环（16次 + 48次）合并为一个64次循环，以消除两次独立的流水线启动/排空开销。
- **方案B**：消除用于同步的 `dup_strm` 模块，将块边界和结束标记作为边带信号（side-band channel）随数据流传递，从而减少固定的握手延迟。
- **方案C**：进行更深度的重构，将 `generateMsgSchedule` 的逻辑内联到 `sha256Digest` 模块中，实现on-the-fly的W值生成，彻底消除一个 `DATAFLOW` stage。
- **方案D**：对64轮核心循环使用 `#pragma HLS pipeline II=1 rewind`，以消除循环间的 prologue/epilogue 气泡。
- **方案E**：增加关键路径上FIFO的深度，并指定其用SRL实现，以缓解潜在的数据流背压。
- 其他还包括预计算、信号通路优化等微调建议。

### 人工审核与采纳情况

模型提供的方案列表展现了良好的广度，为后续工作提供了清晰的路线图。通过分析，我决定采用增量迭代的优化方法，从风险最低、收益最明确的方案开始着手。我选择了方案B（消除同步开销）和**B**方案E（加深FIFO）作为第一批实施的优化点，编写了对应的代码。

---

## 使用场景 3**：关键函数实现**

### **Prompt设计：**

请提供方案B+E的C++实现。要求：

1.请重构`generateMsgSchedule`和`sha256Digest`的函数签名，移除`nblk_strm`和`end_nblk_strm`。

2.请更新`sha256_top`顶层函数，移除对`dup_strm`模块的调用，并正确连接新的边带信号。

3.请在`sha256_top`中，为关键的`hls::stream`数组（`w_lane`和`hash_lane`）提供使用`FIFO_SRL`实现深度缓冲的pragma指令。

4.在每个修改后的函数顶部，添加版本号和简要功能注释。

### 模型输出摘要

模型根据指令生成了应用方案B和方案E后的代码补丁。

### 人工审核与采纳情况

模型生成的代码在结构和接口上基本正确，但在`sha256Digest_onW`函数的循环控制逻辑中引入了死锁风险。初版代码在`sha256Digest_onW`的`do-while`循环中，将`blk_last = w_blk_last_strm.read()`操作放在了64轮迭代**之后**。在Csim中，这并未立即暴露问题。但在进行Cosim时，我发现当处理多块消息时，系统会在处理完第一块后挂起。我进行了代码修正，修正后Cosim顺利PASS。

---

## 使用场景 4**：再次分析P&R并定位气泡**

在获得初步优化结果后，我对报告进行了审查，并发现：核心循环的实际周期（66）与理论周期（64）存在2个周期的微小差异。

### **Prompt设计：**

我已应用了上一轮的优化。分析报告发现一个性能疑点：

- **瓶颈描述**：`generateMsgSchedule`和`sha256Digest`内部的64次核心循环，其报告Latency均为66个周期，而非理论上的64周期。
- **问题**：请解释这额外的2个周期来源于何处，并提出最直接的HLS pragma解决方案来消除这种开销。

### 模型输出摘要

模型指出，当一个循环被连续调用时，标准流水线会在每次调用间产生空闲周期。llm建议，对这两个循环应用`#pragma HLS pipeline II=1 rewind`指令，以实现“回卷流水线”，消除该开销。

### 人工审核与采纳情况

采纳rewind建议，气泡顺利消除。

---

## 使用场景 5**：优化策略转型：从Latency转向Timing**

在II达到理论下限值，latency接近优化极限后，我引导llm进行了优化策略的转向。

### **Prompt设计：**

我们进行策略转型，在不恶化Latency和II的前提下，尽可能降低`Estimated Clock Period`。

请为我提供针对性的优化策略，以代码补丁的形式交付，并满足以下要求：

1. 尽量保持已优化完成的latency和II不恶化。
2. 用 **LaTeX** 格式，为每个函数提供其原始表达式和优化后的表达式，并解释为什么新表达式能降低组合逻辑深度。
3. 请对上述优化的综合效果进行量化预估。

### 模型输出摘要

模型给出2处小而有效的降路径补丁：

在最深的单拍路径上：

1. 用更浅的布尔代数表达式重写 `CH/MAJ`；
2. 把 `Σ0/Σ1` 的三输入 XOR 显式写成两级平衡 XOR 树；

### 人工审核与采纳情况

最初的提示词没有明确要求"用LaTeX格式"和"解释每个步骤的目的和意义"，导致AI提
供的数学推导不够规范和完整。我们修改提示词，特别强调"所有公式使用LaTeX格
式"和"用简明语言解释每个策略的目的和意义"。第二次响应包含了完整的LaTeX公式
和清晰的步骤解释，使我们能够准确理解算法的数学基础。

采纳补丁后`Estimated Period`成功从12.88ns降低到12.55ns。

---

## 使用场景 6**：PrePocessing模块的优化空间分析**

在大幅优化策略进行完成后，我们开始探索小幅优化空间。

### **Prompt设计：**

现在，我们需要对`preProcessing`模块进行一次专项分析。

1. **瓶颈定性**：在当前的Dataflow架构中，判断`preProcessing`模块是“Latency敏感型”还是“Timing敏感型”。
2. **策略选择**：基于上述定性，请明确指出，针对`preProcessing`的优化，以下哪种策略对全局评分的提升更大，并阐述理由：
A) 在Estimated不恶化的情况下，进一步降低其Latency。
B) 在Latency不恶化的情况下，通过简化逻辑来降低其对全局Estimated Period的贡献。

### 模型输出摘要

模型对`preProcessing`模块进行了深入分析，给出了明确结论：由于它处于`DATAFLOW`的“快车道”，不是Latency瓶颈，因此它属于“Timing敏感型”模块。结论是，应采用策略B。

### 人工审核与采纳情况

采纳结论，回退对`preProcessing`模块的大幅重构，仅采用rewind消气泡。

---

## **总结**

### **整体贡献度评估**

- **大模型在本项目中的总体贡献占比**：约 **40%**
- **主要帮助领域**：架构方案探索、HLS pragma专业指导、性能瓶颈分析、可编译代码生成。
- **人工介入与修正比例**：约 **60%**。我的核心贡献在于：
    1. **制定与调整顶层战略**：在关键节点决策优化重心的转移。
    2. **方案筛选与决策**：从模型提供的多种可能性中，结合风险、收益和项目阶段，选择最合适的实施路径。
    3. **批判性审核与实证**：对模型的建议进行独立的仿真和综合验证，并基于实测数据进行最终决策。
    4. **闭环反馈与指导**：在AI方案不达预期时，提供数据驱动的反馈，主导其进行方案修正。

### **学习收获**

通过与大模型进行深度、专业的交互，我不仅高效地完成了复杂的HLS性能优化，更在实践中巩固了从顶层架构设计到微观代码调优的全链路能力。这次经历证明，将个人开发中的工程经验与大模型的广博知识和快速生成能力相结合，是未来高性能硬件设计与创新的高效范式。